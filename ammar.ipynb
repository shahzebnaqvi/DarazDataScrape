{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "filename = \"main2.csv\"\n",
    "f = open(filename, \"w\",encoding=\"utf-8\")\n",
    "headers = \" Brand/Seller, Product_Name, Price, SKU/ID , Category Flow \\n\"\n",
    "f.write(headers)\n",
    "\n",
    "\n",
    "\n",
    "prime_url = \"https://www.daraz.com.pk/\"\n",
    "while True:\n",
    "    try:\n",
    "        resource = requests.get(prime_url).text\n",
    "        break\n",
    "    except:\n",
    "        print(\"wait for 10 sec folk\")\n",
    "        time.wait(10)\n",
    "soup = BeautifulSoup(resource,\"html.parser\")\n",
    "\n",
    "list = soup.findAll(\"li\",{\"class\":\"menu-item\"})\n",
    "cat_list_urls = []\n",
    "brand_urls = []\n",
    "brand_names = []\n",
    "for i in range(len(list)):\n",
    "    cat_list = list[i].findAll(\"a\",{\"class\":\"category\"})\n",
    "    for i in range(len(cat_list)):\n",
    "        cat_list_urls.append(cat_list[i]['href'])\n",
    "#print(cat_list_urls)   #every catagory url is in this array,\n",
    "\n",
    "for i in range(len(cat_list_urls)-1):\n",
    "    cat_list_url = cat_list_urls[i];  #cat_list_url[0] means men fashion's western category , should be cat_list_urls[i]\n",
    "    if( cat_list_url == \"https://www.daraz.com.bd/womens-clothing-top-brands/\" or cat_list_url == \"https://www.daraz.com.bd/womens-fashion/?sort=newest&dir=desc\" or cat_list_url == \"https://www.daraz.com.bd/womens-fashion/?special_price=1\"):\n",
    "        continue\n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                cat_resource = requests.get(cat_list_url).text\n",
    "                break\n",
    "            except:\n",
    "                print(\"wait for 10 sec folk\")\n",
    "                time.wait(10)\n",
    "\n",
    "        cat_soup = BeautifulSoup(cat_resource,\"html.parser\")\n",
    "\n",
    "        brand_article = cat_soup .findAll(\"article\",{\"class\":\"ft-vertical-filter-brand\"})\n",
    "        brand_div = brand_article[0].findAll(\"div\",{\"class\":\"-facet\"})\n",
    "        brand_anchor = brand_div[0].findAll(\"a\",{\"class\":\"facet-link\"})  #brand_anchor is an array of anchor tags containing the urls and names, under the MEN>WESTERN CATEGORY\n",
    "\n",
    "        for i in range(len(brand_anchor)-1):\n",
    "            brand_urls.append(brand_anchor[i]['href'])\n",
    "            brand_names.append(brand_anchor[i]['title'])\n",
    "\n",
    "        for i in range(len(brand_urls)-1):\n",
    "            brand_url =  brand_urls[i]\n",
    "            for i in range(25+1):\n",
    "                page_string = \"?page=\"\n",
    "                url = brand_url + page_string + str(i+1)\n",
    "                print(url)\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        resource = requests.get(url).text\n",
    "                        break\n",
    "                    except:\n",
    "                        print(\"wait for 10 sec folk\")\n",
    "                        time.wait(10)\n",
    "\n",
    "                soup = BeautifulSoup(resource,\"html.parser\")\n",
    "                chronology = soup.findAll(\"nav\",{\"class\":\"osh-breadcrumb\"})\n",
    "\n",
    "\n",
    "\n",
    "                if(len(chronology) ==0 ) :\n",
    "                    break\n",
    "\n",
    "\n",
    "                cat_flow_section = soup.findAll(\"section\",{\"class\":\"products\"})\n",
    "                cat_flow_div = cat_flow_section[0].findAll(\"div\",{\"class\":\"-gallery\"})\n",
    "                cat_flow_link = []\n",
    "                for i in range(len(cat_flow_div)):\n",
    "                    cat_flow_link.append(cat_flow_div[i].a['href'])\n",
    "\n",
    "\n",
    "\n",
    "                ulist_item = chronology[0].ul\n",
    "\n",
    "                b = ulist_item.findAll(\"li\",{\"class\":\"last-child -brand\"})\n",
    "                brand = b[0].a.contents\n",
    "                print(\"brand: \" +brand[0])  #######################################################\n",
    "\n",
    "\n",
    "                products = soup.findAll(\"a\",{\"class\":\"link\"})\n",
    "\n",
    "                for i in range(len(products)):\n",
    "                    product = products[i].h2\n",
    "                    n = product.findAll(\"span\",{\"class\":\"name\"})\n",
    "                    product_name = n[0].contents\n",
    "                    print(\"product_name: \" +product_name[0])   ##################################################\n",
    "\n",
    "                    price_container = products[i].findAll(\"div\",{\"class\":\"price-container\"})\n",
    "                    span_class_price = price_container[0].findAll(\"span\",{\"class\":\"price-box\"})\n",
    "                    span_tags = span_class_price[0].span.find_all('span')\n",
    "                    spantag_data_price = span_tags[1]\n",
    "                    price = spantag_data_price['data-price']\n",
    "                    print(\"price: \" +price) #################################################\n",
    "\n",
    "                    id_holder = products[i].div\n",
    "                    id = id_holder.img['data-sku']\n",
    "                    print(\"id: \" + id)    #########################################\n",
    "\n",
    "                    while True:\n",
    "                        try:\n",
    "                            cat_flow_resource = requests.get(cat_flow_link[i]).text\n",
    "                            break\n",
    "                        except:\n",
    "                            print(\"wait for 10 sec folk\")\n",
    "                            time.wait(10)\n",
    "                    cat_flow_soup = BeautifulSoup(cat_flow_resource,\"html.parser\")\n",
    "                    cat_flow_nav = cat_flow_soup.findAll(\"nav\",{\"class\":\"osh-breadcrumb\"})\n",
    "                    cat_nav_ul = cat_flow_nav[0].ul\n",
    "                    cat_nav_li = cat_nav_ul.findAll(\"li\",{\"class\":\"\"})\n",
    "                    flow_names = \" \"\n",
    "                    for i in range(len(cat_nav_li)-1):\n",
    "                        flow = cat_nav_li[i].a.contents\n",
    "                        flow_names = flow_names+ flow[0] +\">\"\n",
    "                    print(flow_names)  ##############################################\n",
    "\n",
    "                    f.write(''.join(brand + [\",\"] + product_name + [\",\"] +[price] + [\",\"] + [id] + [\",\"] + [flow_names] +[\",\"] + [\"\\n\"]) )\n",
    "\n",
    "f.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                                             stdin=PIPE)\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fa459f47df73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://web.whatsapp.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[1;32m---> 83\u001b[1;33m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[0;32m     84\u001b[0m                 )\n\u001b[0;32m     85\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://web.whatsapp.com/')\n",
    "\n",
    "name = input('Enter the name of user or group : ')\n",
    "msg = input('Enter your message : ')\n",
    "count = int(input('Enter the count : '))\n",
    "\n",
    "input('Enter anything after scanning QR code')\n",
    "\n",
    "user = driver.find_element_by_xpath('//span[@title = \"{}\"]'.format(name))\n",
    "user.click()\n",
    "\n",
    "msg_box = driver.find_element_by_class_name('_3u328')\n",
    "\n",
    "for i in range(count):\n",
    "    msg_box.send_keys(msg)\n",
    "    button = driver.find_element_by_class_name('_3M-N-')\n",
    "    button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "resp = requests.get(url)\n",
    "resp\n",
    "data = resp.json()\n",
    "data[0]['title']\n",
    "issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state'])\n",
    "issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent' : 'chrome/ 83.0.4103.97'\n",
    "}\n",
    "res = requests.get('https://www.daraz.com.np/catalog/?spm=a2a0e.searchlistcategory.search.2.3eac4b8amQJ0zd&q=samsung%20m20&_keyori=ss&from=suggest_normal&sugg=samsung%20m20_1_1', headers = headers)\n",
    "soup = bs(res.content, 'lxml')\n",
    "r = re.compile(r'window.pageData=(.*)')\n",
    "data = soup.find('script', text=r).text\n",
    "script = r.findall(data)[0]\n",
    "items = json.loads(script)['mods']['listItems']\n",
    "results = []\n",
    "\n",
    "for item in items:\n",
    "    row = [item['name'], item['priceShow'], item['productUrl'], item['ratingScore']]\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Name', 'Price', 'ProductUrl', 'Rating'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name    Price  \\\n",
      "0              Back Tpu Screen Guard For Samsung M20  Rs. 300   \n",
      "1  Impact 9H Full Glue Tempered Glass For Samsung...  Rs. 190   \n",
      "2  For Samsung Galaxy M20 5D 9H Full Glue Tempere...  Rs. 450   \n",
      "\n",
      "                                          ProductUrl Rating  \n",
      "0  //www.daraz.com.np/products/back-tpu-screen-gu...      0  \n",
      "1  //www.daraz.com.np/products/impact-9h-full-glu...      5  \n",
      "2  //www.daraz.com.np/products/for-samsung-galaxy...      5  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0'\n",
    "}\n",
    "res = requests.get('https://www.daraz.com.np/catalog/?spm=a2a0e.searchlistcategory.search.2.3eac4b8amQJ0zd&q=samsung%20m20&_keyori=ss&from=suggest_normal&sugg=samsung%20m20_1_1', headers = headers)\n",
    "soup = bs(res.content, 'lxml')\n",
    "for script in soup.select('script'):\n",
    "    if 'window.pageData=' in script.text:\n",
    "        script = script.text.replace('window.pageData=','')\n",
    "        break\n",
    "items = json.loads(script)['mods']['listItems']\n",
    "results = []\n",
    "\n",
    "for item in items:\n",
    "    #print(item)\n",
    "    #extract other info you want\n",
    "    row = [item['name'], item['priceShow'], item['productUrl'], item['ratingScore']]\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Name', 'Price', 'ProductUrl', 'Rating'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.daraz.com.np/catalog/?spm=a2a0e.searchlistcategory.search.2.3eac4b8amQJ0zd&q=samsung%20m20&_keyori=ss&from=suggest_normal&sugg=samsung%20m20_1_1\")\n",
    "time.sleep(5)\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "parsed_html = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "containers = parsed_html.find_all(\"div\", {\"class\" : \"c2prKC\"})\n",
    "\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import ssl\n",
    "import requests\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "html =  urlopen(\"https://www.daraz.com.pk\")\n",
    "bsObj = BeautifulSoup(html, features=\"lxml\")\n",
    "nameList = bsObj.find(\"input\", {\"type\": \"search\"})\n",
    "\n",
    "print(nameList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html =  urlopen(\"https://www.amazon.com\")\n",
    "bsObj = BeautifulSoup(html, features=\"lxml\")\n",
    "nameList = bsObj.find(\"input\", {\"type\": \"text\"})\n",
    "print(nameList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.daraz.com.pk\")\n",
    "\n",
    "time.sleep(2)\n",
    "content = driver.page_source.encode('utf-8').strip()\n",
    "soup = BeautifulSoup(content,\"html.parser\")\n",
    "time.sleep(2)\n",
    "officials = soup.find(\"input\", {\"type\":\"search\"})\n",
    "\n",
    "print(str(officials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from scrapy_splash import SplashRequest\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class DarazsSpider(scrapy.Spider):\n",
    "    name = 'darazs'\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_TIMEOUT': 60000,\n",
    "        'DOWNLOAD_MAXSIZE': 0,\n",
    "        'DOWNLOAD_WARNSIZE': 0\n",
    "    }\n",
    "    script = '''\n",
    "function main(splash, args)\n",
    "assert(splash:go(args.url))\n",
    "assert(splash:wait(0.5))\n",
    "treat=require('treat')\n",
    "result = {}\n",
    "for i=2,68,1\n",
    "    do\n",
    "        assert(splash:runjs('document.querySelector(\".ant-pagination-next > a\").click()'))\n",
    "        assert(splash:wait(5.0))\n",
    "        result[i]=splash:html()\n",
    "\n",
    "\n",
    "    end  \n",
    "return treat.as_array(result)\n",
    "end\n",
    "    '''\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'https://www.daraz.pk/smartphones/'\n",
    "        yield SplashRequest(url=url, callback=self.parse, endpoint='render.html', args={'wait': 0.5})\n",
    "        yield SplashRequest(url=url, callback=self.parse_other_pages, endpoint='execute',\n",
    "                            args={'wait': 0.5, 'lua_source': self.script, 'timeout': 3600}, dont_filter=True)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for phone in response.xpath('//div[@class=\"c2prKC\"]'):\n",
    "            yield {\n",
    "                'Name': phone.xpath('.//div[@class=\"c16H9d\"]/a/text()').extract(),\n",
    "                'Price': phone.xpath('.//span[@class=\"c13VH6\"]/text()').extract(),\n",
    "                'old_price': phone.xpath('.//del[@class=\"c13VH6\"]/text()').extract(),\n",
    "\n",
    "            }\n",
    "\n",
    "    def parse_other_pages(self, response):\n",
    "        for page in response.data:\n",
    "            sel = Selector(text=page, type=None, root=None)\n",
    "            for phone in sel.xpath('//div[@class=\"c2prKC\"]'):\n",
    "                yield {\n",
    "                    'Name': phone.xpath('.//div[@class=\"c16H9d\"]/a/text()').extract(),\n",
    "                    'Price': phone.xpath('.//span[@class=\"c13VH6\"]/text()').extract(),\n",
    "                    'old_price': phone.xpath('.//del[@class=\"c13VH6\"]/text()').extract(),\n",
    "\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "filename = \"main2.csv\"\n",
    "f = open(filename, \"w\",encoding=\"utf-8\")\n",
    "headers = \" Brand/Seller, Product_Name, Price, SKU/ID , Category Flow \\n\"\n",
    "f.write(headers)\n",
    "\n",
    "\n",
    "\n",
    "prime_url = \"https://www.daraz.com.bd/\"\n",
    "while True:\n",
    "    try:\n",
    "        resource = requests.get(prime_url).text\n",
    "        break\n",
    "    except:\n",
    "        print(\"wait for 10 sec folk\")\n",
    "        time.wait(10)\n",
    "soup = BeautifulSoup(resource,\"html.parser\")\n",
    "list = soup.findAll(\"li\",{\"class\":\"menu-item\"})\n",
    "cat_list_urls = []\n",
    "brand_urls = []\n",
    "brand_names = []\n",
    "for i in range(len(list)):\n",
    "    cat_list = list[i].findAll(\"a\",{\"class\":\"category\"})\n",
    "    for i in range(len(cat_list)):\n",
    "        cat_list_urls.append(cat_list[i]['href'])\n",
    "#print(cat_list_urls)   #every catagory url is in this array,\n",
    "\n",
    "for i in range(len(cat_list_urls)-1):\n",
    "    cat_list_url = cat_list_urls[i];  #cat_list_url[0] means men fashion's western category , should be cat_list_urls[i]\n",
    "    if( cat_list_url == \"https://www.daraz.com.bd/womens-clothing-top-brands/\" or cat_list_url == \"https://www.daraz.com.bd/womens-fashion/?sort=newest&dir=desc\" or cat_list_url == \"https://www.daraz.com.bd/womens-fashion/?special_price=1\"):\n",
    "        continue\n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                cat_resource = requests.get(cat_list_url).text\n",
    "                break\n",
    "            except:\n",
    "                print(\"wait for 10 sec folk\")\n",
    "                time.wait(10)\n",
    "\n",
    "        cat_soup = BeautifulSoup(cat_resource,\"html.parser\")\n",
    "\n",
    "        brand_article = cat_soup .findAll(\"article\",{\"class\":\"ft-vertical-filter-brand\"})\n",
    "        brand_div = brand_article[0].findAll(\"div\",{\"class\":\"-facet\"})\n",
    "        brand_anchor = brand_div[0].findAll(\"a\",{\"class\":\"facet-link\"})  #brand_anchor is an array of anchor tags containing the urls and names, under the MEN>WESTERN CATEGORY\n",
    "\n",
    "        for i in range(len(brand_anchor)-1):\n",
    "            brand_urls.append(brand_anchor[i]['href'])\n",
    "            brand_names.append(brand_anchor[i]['title'])\n",
    "\n",
    "        for i in range(len(brand_urls)-1):\n",
    "            brand_url =  brand_urls[i]\n",
    "            for i in range(25+1):\n",
    "                page_string = \"?page=\"\n",
    "                url = brand_url + page_string + str(i+1)\n",
    "                print(url)\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        resource = requests.get(url).text\n",
    "                        break\n",
    "                    except:\n",
    "                        print(\"wait for 10 sec folk\")\n",
    "                        time.wait(10)\n",
    "\n",
    "                soup = BeautifulSoup(resource,\"html.parser\")\n",
    "                chronology = soup.findAll(\"nav\",{\"class\":\"osh-breadcrumb\"})\n",
    "\n",
    "\n",
    "\n",
    "                if(len(chronology) ==0 ) :\n",
    "                    break\n",
    "\n",
    "\n",
    "                cat_flow_section = soup.findAll(\"section\",{\"class\":\"products\"})\n",
    "                cat_flow_div = cat_flow_section[0].findAll(\"div\",{\"class\":\"-gallery\"})\n",
    "                cat_flow_link = []\n",
    "                for i in range(len(cat_flow_div)):\n",
    "                    cat_flow_link.append(cat_flow_div[i].a['href'])\n",
    "\n",
    "\n",
    "\n",
    "                ulist_item = chronology[0].ul\n",
    "\n",
    "                b = ulist_item.findAll(\"li\",{\"class\":\"last-child -brand\"})\n",
    "                brand = b[0].a.contents\n",
    "                print(\"brand: \" +brand[0])  #######################################################\n",
    "\n",
    "\n",
    "                products = soup.findAll(\"a\",{\"class\":\"link\"})\n",
    "\n",
    "                for i in range(len(products)):\n",
    "                    product = products[i].h2\n",
    "                    n = product.findAll(\"span\",{\"class\":\"name\"})\n",
    "                    product_name = n[0].contents\n",
    "                    print(\"product_name: \" +product_name[0])   ##################################################\n",
    "\n",
    "                    price_container = products[i].findAll(\"div\",{\"class\":\"price-container\"})\n",
    "                    span_class_price = price_container[0].findAll(\"span\",{\"class\":\"price-box\"})\n",
    "                    span_tags = span_class_price[0].span.find_all('span')\n",
    "                    spantag_data_price = span_tags[1]\n",
    "                    price = spantag_data_price['data-price']\n",
    "                    print(\"price: \" +price) #################################################\n",
    "\n",
    "                    id_holder = products[i].div\n",
    "                    id = id_holder.img['data-sku']\n",
    "                    print(\"id: \" + id)    #########################################\n",
    "\n",
    "                    while True:\n",
    "                        try:\n",
    "                            cat_flow_resource = requests.get(cat_flow_link[i]).text\n",
    "                            break\n",
    "                        except:\n",
    "                            print(\"wait for 10 sec folk\")\n",
    "                            time.wait(10)\n",
    "                    cat_flow_soup = BeautifulSoup(cat_flow_resource,\"html.parser\")\n",
    "                    cat_flow_nav = cat_flow_soup.findAll(\"nav\",{\"class\":\"osh-breadcrumb\"})\n",
    "                    cat_nav_ul = cat_flow_nav[0].ul\n",
    "                    cat_nav_li = cat_nav_ul.findAll(\"li\",{\"class\":\"\"})\n",
    "                    flow_names = \" \"\n",
    "                    for i in range(len(cat_nav_li)-1):\n",
    "                        flow = cat_nav_li[i].a.contents\n",
    "                        flow_names = flow_names+ flow[0] +\">\"\n",
    "                    print(flow_names)  ##############################################\n",
    "\n",
    "                    f.write(''.join(brand + [\",\"] + product_name + [\",\"] +[price] + [\",\"] + [id] + [\",\"] + [flow_names] +[\",\"] + [\"\\n\"]) )\n",
    "\n",
    "f.close()\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import shutil\n",
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "import string\n",
    "#  c1DXz4  items amount \n",
    "\n",
    "#  c1_t2i products items\n",
    "\n",
    "#  c3e8SH > c5TXIP > cRjKsc  a >img \n",
    "\n",
    "#  c3e8SH > c5TXIP > c3KeDq > c16H9d > a = title\n",
    "\n",
    "#  c3e8SH > c5TXIP > c3KeDq > c3gUW0 > span = price\n",
    "\n",
    "# c3e8SH > c5TXIP > c3KeDq > c15YQ9  > c2JB4x c6Ntq9 > i = ratings\n",
    "\n",
    "#  c3e8SH > c5TXIP > c3KeDq > c15YQ9 > span = location \n",
    "import random\n",
    "import json\n",
    "\n",
    "tracks = []\n",
    "def id_generator(size=6, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "def download_image(image):\n",
    "    url = image\n",
    "    data = url.split('/')\n",
    "    response = requests.get(url, stream=True)\n",
    "    realname = data[-1]\n",
    "    if realname in tracks:\n",
    "        realname = id_generator()+realname\n",
    "    tracks.append(realname)\n",
    "    file = open(\"./images//iphone//{}.jpg\".format(realname), 'wb')\n",
    "    response.raw.decode_content = True\n",
    "    shutil.copyfileobj(response.raw, file)\n",
    "    del response\n",
    "\n",
    "\n",
    "def scraper(url):\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.content,'html5lib')\n",
    "    data = (soup.find_all('script', type='application/ld+json'))\n",
    "    #print(data[1].text)\n",
    "    data = json.loads(data[1].text)\n",
    "\n",
    "    offers = (data['itemListElement'])\n",
    "    #print(offers)\n",
    "    products = []\n",
    "    for offer in offers:\n",
    "        product ={}\n",
    "        for key,value in offer.items():\n",
    "            if key == 'offers':\n",
    "                for k,v in value.items():\n",
    "                    product[k] = v\n",
    "            else:\n",
    "                product[key] = value\n",
    "        products.append(product)\n",
    "    print('total products fetched so far '+ str(len(products)) )\n",
    "    writetocsv(products,'daraz_iphone.csv')\n",
    "\n",
    "\n",
    "def writetocsv(products,filename):\n",
    "    with open(filename, 'w') as f: \n",
    "        header=['priceCurrency','@type','price','availability','image','name','url','']\n",
    "        w = csv.DictWriter(f,header) \n",
    "        w.writeheader() \n",
    "        for product in products:\n",
    "            image = product['image']\n",
    "            download_image(image) \n",
    "            w.writerow(product)\n",
    "\n",
    "    \n",
    "\n",
    "url = \"https://www.daraz.com.bd/\";\n",
    "#scraper(url)\n",
    "\n",
    "data = requests.get(url).text;\n",
    "soup = BeautifulSoup(data.content,'ht')\n",
    "list = soup.findAll(\"li\",{\"class\":\"menu-item\"})\n",
    "print(list)\n",
    "#result = soup.findAll(\"a\", {\"class\":\"category\"})\n",
    "    \n",
    "#print (result)\n",
    "\n",
    "#print('daraz scrapped successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path = 'datasets/bitly_usagov/example.txt'\n",
    "records = [json.loads(line) for line in open(path)]\n",
    "records[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-6-252a84828242>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-252a84828242>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    dynamic_text = soup.find_all(\"p\", {\"class\":\"class_name\"}) #or other attributes, optional\u001b[0m\n\u001b[1;37m                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://somedomain/url_that_delays_loading\")\n",
    "try:\n",
    "    element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"myDynamicElement\"))) #waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, \"lxml\")\n",
    "    dynamic_text = soup.find_all(\"p\", {\"class\":\"class_name\"}) #or other attributes, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0ccd03653a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests_html\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-4ec825f3671e>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-4ec825f3671e>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    response = requests.get(url.format(page, PAGE_SIZE, letter))\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import string\n",
    "PAGE_SIZE = 15\n",
    "url = 'http://example.webscraping.com/ajax/' + 'search.json?page={}&page_size={}&search_term=a'\n",
    "countries = set()\n",
    "for letter in string.ascii_lowercase:\n",
    "    print('Searching with %s' % letter)\n",
    "    page = 0\n",
    "    while True:\n",
    "    response = requests.get(url.format(page, PAGE_SIZE, letter))\n",
    "    data = response.json()\n",
    "    print('adding %d records from the page %d' %(len(data.get('records')),page))\n",
    "    for record in data.get('records'):countries.add(record['country'])\n",
    "    page += 1\n",
    "    if page >= data['num_pages']:\n",
    "        break\n",
    "    with open('countries.txt', 'w') as countries_file:\n",
    "    countries_file.write('n'.join(sorted(countries))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import HTMLSession from requests_html\n",
    "from requests_html import HTMLSession\n",
    " \n",
    "# create an HTML Session object\n",
    "session = HTMLSession()\n",
    " \n",
    "# Use the object above to connect to needed webpage\n",
    "resp = session.get(\"https://finance.yahoo.com/quote/NFLX/options?p=NFLX\")\n",
    " \n",
    "# Run JavaScript code on webpage\n",
    "resp.html.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "# specify the url\n",
    "urlpage = 'https://www.daraz.pk/products/children-portable-multipurpuse-plastic-table-for-children-2-in-1-study-for-kids-i132565493-s1292778436.html?spm=a2a0e.home.just4u.5.4ce84937EYJDWD&scm=1007.28811.164133.0&pvid=ed675cb2-b410-4026-bc0f-d6af4e16b33b&clickTrackInfo=pvid%3Aed675cb2-b410-4026-bc0f-d6af4e16b33b%3Bchannel_id%3A0000%3Bmt%3Ahot%3Bitem_id%3A132565493%3B' \n",
    "print(urlpage)\n",
    "# query the website and return the html to the variable 'page'\n",
    "page = urllib.request.urlopen(urlpage)\n",
    "# parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "# find product items\n",
    "# at time of publication, Nov 2018:\n",
    "# results = soup.find_all('div', attrs={'class': 'listing category_templates clearfix productListing'})\n",
    "# updated Nov 2019:\n",
    "results = soup.find_all('div', attrs={'class': 'card-jfy-ratings-comment'})\n",
    "print('Number of results', len(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "# specify the url\n",
    "urlpage = 'https://groceries.asda.com/search/yogurt' \n",
    "print(urlpage)\n",
    "# run firefox webdriver from executable path of your choice\n",
    "driver = webdriver.Firefox(executable_path = 'c/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json library\n",
    "import json\n",
    "# request url\n",
    "urlreq = 'https://groceries.asda.com/api/items/search?keyword=yogurt'\n",
    "# get response\n",
    "response = urllib.request.urlopen(urlreq)\n",
    "# load as json\n",
    "jresponse = json.load(response)\n",
    "# write to file as pretty print\n",
    "with open('asdaresp.json', 'w') as outfile:\n",
    "    json.dump(jresponse, outfile, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch url\n",
    "url = \"http://kanview.ks.gov/PayRates/PayRates_Agency.aspx\"\n",
    "\n",
    "# create a new Firefox session\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url)\n",
    "\n",
    "python_button = driver.find_element_by_id('MainContent_uxLevel1_Agencies_uxAgencyBtn_33') #FHSU\n",
    "python_button.click() #click fhsu link\n",
    "#Selenium hands the page source to Beautiful Soup\n",
    "soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "datalist = [] #empty list\n",
    "x = 0 #counter\n",
    "\n",
    "for link in soup_level1.find_all('a', id=re.compile(\"^MainContent_uxLevel2_JobTitles_uxJobTitleBtn_\")):\n",
    "    ##code to execute in for loop goes here\n",
    "#Beautiful Soup grabs all Job Title links\n",
    "for link in soup_level1.find_all('a', id=re.compile(\"^MainContent_uxLevel2_JobTitles_uxJobTitleBtn_\")):\n",
    "    \n",
    "    #Selenium visits each Job Title page\n",
    "    python_button = driver.find_element_by_id('MainContent_uxLevel2_JobTitles_uxJobTitleBtn_' + str(x))\n",
    "    python_button.click() #click link\n",
    "    \n",
    "    #Selenium hands of the source of the specific job page to Beautiful Soup\n",
    "    soup_level2=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    #Beautiful Soup grabs the HTML table on the page\n",
    "    table = soup_level2.find_all('table')[0]\n",
    "    \n",
    "    #Giving the HTML table to pandas to put in a dataframe object\n",
    "    df = pd.read_html(str(table),header=0)\n",
    "    \n",
    "    #Store the dataframe in a list\n",
    "    datalist.append(df[0])\n",
    "    \n",
    "    #Ask Selenium to click the back button\n",
    "    driver.execute_script(\"window.history.go(-1)\") \n",
    "    \n",
    "    #increment the counter variable before starting the loop over\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name    Price  \\\n",
      "0   High Quality Handfree Extra Bass Best for PUBG...  Rs. 399   \n",
      "1   Best Quality Woofer headphones Super Basser/ H...  Rs. 275   \n",
      "2   Ronins R725 / R-725 ORIGINAL Handfree Extreme ...  Rs. 675   \n",
      "3   Samsung Galaxy S6 S7 Original headphone Handfr...  Rs. 189   \n",
      "4   Universal Wireless Bluetooth Handfree Wireless...  Rs. 625   \n",
      "5   Original i12 TWS Wireless Bluetooth 5.0 Earpho...  Rs. 840   \n",
      "6   Original VlVO Handfree Branded High Quality Su...  Rs. 399   \n",
      "7   Blue tooth Mini Stereo Wireless Microphone Mic...  Rs. 355   \n",
      "8     Blutooth wired handfree for iphone 7,8,X and 11  Rs. 399   \n",
      "9   Airox Handfree Stereo Headset Headsfree High Q...  Rs. 250   \n",
      "10  Top Trending Mini S530 Wireless Bluetooth Earp...  Rs. 333   \n",
      "11  Bluetooth Handfree Wireless Bluetooth Headset ...  Rs. 240   \n",
      "12  IPhone 100% Original Handfree Earphones 3.55mm...  Rs. 274   \n",
      "13  Original Gionee Hand free With Great Woofer & ...  Rs. 599   \n",
      "14  i9s i7s tws True wireless stereo Bluetooth Ear...  Rs. 699   \n",
      "15  Original OPPO Handfree Branded High Quality Su...  Rs. 249   \n",
      "16  Apple Handfree Earphones Headphones With Mic S...  Rs. 249   \n",
      "17  Lamyik L29 Wired Earphone Handfree High Bass S...  Rs. 350   \n",
      "18  [100% Genuine] Samsung A8,A7,A5,J7,J5,J3,J2 In...  Rs. 270   \n",
      "19  Woofer headphones / Handfree Super Basser With...  Rs. 328   \n",
      "20     Eid Sale Earphones Handfree Stereo Base -White  Rs. 198   \n",
      "21  Original Gionee High Quality Super Bass Stereo...  Rs. 399   \n",
      "22  AKG 100% S10 handfree for Samsung galaxy S10, ...  Rs. 350   \n",
      "23  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "24  Special Handfree With 1 Year Warrenty 3.55 Jac...  Rs. 299   \n",
      "25  R-12 Ronins R-12 / R12 Original Handfree Maste...  Rs. 295   \n",
      "26                                      AKG handsfree  Rs. 435   \n",
      "27  Original Universal Handfree Stereo Multi Brand...  Rs. 249   \n",
      "28  i7 TWS Twins Wireless Earbuds / Bluetooth Head...  Rs. 649   \n",
      "29  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "30  i9s i7s tws True wireless stereo Bluetooth Ear...  Rs. 699   \n",
      "31  Original Mini S530 Wireless Bluetooth Earphone...  Rs. 333   \n",
      "32  Original Universal Handfree Stereo  VlVo  Wire...  Rs. 299   \n",
      "33  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "34  Original Universal Handfree Stereo Multi Brand...  Rs. 249   \n",
      "35  i9s i7s tws True wireless stereo Bluetooth Ear...  Rs. 699   \n",
      "36  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "37  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "38  M5 Wireless Sport Bluetooth Handsfree Magnetic...  Rs. 333   \n",
      "39  i7 TWS ( Twins ) Wireless Earbuds Mini Bluetoo...  Rs. 620   \n",
      "\n",
      "                                           ProductUrl   Rating  \n",
      "0   //www.daraz.pk/products/high-quality-handfree-...        5  \n",
      "1   //www.daraz.pk/products/best-quality-woofer-he...        4  \n",
      "2   //www.daraz.pk/products/ronins-r725-r-725-orig...        0  \n",
      "3   //www.daraz.pk/products/samsung-galaxy-s6-s7-o...        0  \n",
      "4   //www.daraz.pk/products/universal-wireless-blu...        0  \n",
      "5   //www.daraz.pk/products/original-i12-tws-wirel...  4.11111  \n",
      "6   //www.daraz.pk/products/original-vlvo-handfree...        0  \n",
      "7   //www.daraz.pk/products/blue-tooth-mini-stereo...        0  \n",
      "8   //www.daraz.pk/products/blutooth-wired-handfre...        1  \n",
      "9   //www.daraz.pk/products/airox-handfree-stereo-...        0  \n",
      "10  //www.daraz.pk/products/top-trending-mini-s530...        0  \n",
      "11  //www.daraz.pk/products/bluetooth-handfree-wir...        0  \n",
      "12  //www.daraz.pk/products/iphone-100-original-ha...      4.5  \n",
      "13  //www.daraz.pk/products/buy-1-gionee-handsfree...        0  \n",
      "14  //www.daraz.pk/products/i9s-i7s-tws-true-wirel...        0  \n",
      "15  //www.daraz.pk/products/original-oppo-handfree...      4.6  \n",
      "16  //www.daraz.pk/products/apple-handfree-earphon...        0  \n",
      "17  //www.daraz.pk/products/lamyik-l29-wired-earph...        5  \n",
      "18  //www.daraz.pk/products/100-genuine-samsung-a8...        5  \n",
      "19  //www.daraz.pk/products/woofer-headphones-hand...        0  \n",
      "20  //www.daraz.pk/products/eid-sale-earphones-han...      4.5  \n",
      "21  //www.daraz.pk/products/original-gionee-high-q...        0  \n",
      "22  //www.daraz.pk/products/akg-100-s10-handfree-f...        0  \n",
      "23  //www.daraz.pk/products/m5-wireless-sport-blue...        2  \n",
      "24  //www.daraz.pk/products/special-handfree-with-...     3.75  \n",
      "25  //www.daraz.pk/products/r-12-ronins-r-12-r12-o...        5  \n",
      "26  //www.daraz.pk/products/samsung-s10-akg-high-b...        0  \n",
      "27  //www.daraz.pk/products/original-universal-han...        5  \n",
      "28  //www.daraz.pk/products/i7-tws-twins-wireless-...        1  \n",
      "29  //www.daraz.pk/products/m5-wireless-sport-blue...        0  \n",
      "30  //www.daraz.pk/products/i9s-i7s-tws-true-wirel...        0  \n",
      "31  //www.daraz.pk/products/original-mini-s530-wir...        0  \n",
      "32  //www.daraz.pk/products/original-universal-han...        0  \n",
      "33  //www.daraz.pk/products/m5-wireless-sport-blue...        1  \n",
      "34  //www.daraz.pk/products/original-universal-han...        5  \n",
      "35  //www.daraz.pk/products/i9s-i7s-tws-true-wirel...        3  \n",
      "36  //www.daraz.pk/products/m5-wireless-sport-blue...        5  \n",
      "37  //www.daraz.pk/products/m5-wireless-sport-blue...        1  \n",
      "38  //www.daraz.pk/products/m5-wireless-sport-blue...        0  \n",
      "39  //www.daraz.pk/products/i7-tws-twins-wireless-...        0  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0'\n",
    "}\n",
    "res = requests.get('https://www.daraz.pk/catalog/?_keyori=ss&from=input&page=2&q=handfree&spm=a2a0e.searchlist.search.go.4341173c30g0dj', headers = headers)\n",
    "soup = bs(res.content, 'lxml')\n",
    "for script in soup.select('script'):\n",
    "    if 'window.pageData=' in script.text:\n",
    "        script = script.text.replace('window.pageData=','')\n",
    "        break\n",
    "items = json.loads(script)['mods']['listItems']\n",
    "results = []\n",
    "\n",
    "for item in items:\n",
    "    #print(item)\n",
    "    #extract other info you want\n",
    "    row = [item['name'], item['priceShow'], item['productUrl'], item['ratingScore']]\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Name', 'Price', 'ProductUrl', 'Rating'])\n",
    "df.to_csv (r'E:\\New folder\\Shahzeb\\Piaic\\fiverproject\\ammar.csv', index = False, header=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
